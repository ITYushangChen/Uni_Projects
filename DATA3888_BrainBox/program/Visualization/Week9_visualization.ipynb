{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1a1b20",
   "metadata": {},
   "source": [
    "# E-Reader Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e329857",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "263988ef",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bbcb2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from decimal import Decimal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters, EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pyautogui\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd026671",
   "metadata": {},
   "source": [
    "# Train the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85bc1e",
   "metadata": {},
   "source": [
    "In this section, we are aiming to read all the data and get a feature matrix for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37575593",
   "metadata": {},
   "source": [
    "## Label Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cff8d",
   "metadata": {},
   "source": [
    "Firstly, read the data and get all of the events selected and labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a77ace55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_movement_ZC(Y, time, windowSize=0.5, thresholdEvents=20, downSampleRate=50):\n",
    "    Y = Y - 500\n",
    "    ind = np.arange(0, np.where(time == np.round(time[-1] - windowSize, 4))[0][0] + 1, downSampleRate)\n",
    "    Y = Y - np.average(Y)\n",
    "    timeMiddle = time[ind] + windowSize / 2\n",
    "    testStat = np.empty(len(ind), dtype=int)\n",
    "    for i in range(len(ind)):\n",
    "        Y_subset = Y[(time >= time[ind[i]]) & (time < time[ind[i]] + windowSize)]\n",
    "        if np.ndim(Y_subset) > 1:\n",
    "            Y_subset = Y_subset[:, 1]\n",
    "        testStat[i] = sum(Decimal(int(Y_subset[i])) * Decimal(int(Y_subset[i + 1])) <= 0 for i in range(len(Y_subset) - 1))\n",
    "    predictedEvent = np.where(testStat < thresholdEvents)[0]\n",
    "    eventTimes = timeMiddle[predictedEvent]\n",
    "    gaps = np.where(np.diff(eventTimes) > windowSize)[0]\n",
    "    if len(eventTimes) == 0:\n",
    "        return None\n",
    "    event_time_interval = [np.min(eventTimes)]\n",
    "    for i in range(len(gaps)):\n",
    "        event_time_interval.extend([eventTimes[gaps[i]], eventTimes[gaps[i] + 1]])\n",
    "    event_time_interval.append(np.max(eventTimes))\n",
    "    event_time_interval = np.reshape(event_time_interval, (-1, 2))\n",
    "\n",
    "    predictedEventTimes = np.full(len(Y), False)\n",
    "    for i in range(event_time_interval.shape[0]):\n",
    "        predictedEventTimes[(event_time_interval[i, 0] <= time) & (event_time_interval[i, 1] >= time)] = True\n",
    "\n",
    "    num_event = len(gaps) + 1\n",
    "    movement_list = []\n",
    "    y_values = []\n",
    "    for i in range(event_time_interval.shape[0]):\n",
    "        interval_start, interval_end = event_time_interval[i]\n",
    "        interval_indices = (time >= interval_start) & (time <= interval_end)\n",
    "        interval_Y = Y[interval_indices]\n",
    "        y_values.append(interval_Y)\n",
    "        movement = LR_detection(interval_Y)\n",
    "        movement_list.append(movement)\n",
    "    return {\n",
    "        \"num_event\": num_event,\n",
    "        \"predictedEventTimes\": predictedEventTimes,\n",
    "        \"predictedInterval\": event_time_interval,\n",
    "        \"labels\": movement_list,\n",
    "        \"signals\": y_values\n",
    "    }\n",
    "\n",
    "def LR_detection(seq):\n",
    "    maxval = np.max(seq)\n",
    "    minval = np.min(seq)\n",
    "    movement = \"L\" if maxval < -minval else \"R\"\n",
    "    return movement\n",
    "def record_all_training(files,method):\n",
    "    ls_signals = []\n",
    "    ls_labels = []\n",
    "    ls_intervals = []\n",
    "    for i in range(len(files)):\n",
    "        wave = files[i]\n",
    "        print(wave)\n",
    "        if wave[-3:] != \"wav\":\n",
    "            continue\n",
    "        window_size, Y = wavfile.read(wave)\n",
    "        timeSeq = []\n",
    "        for i in range(len(Y)):\n",
    "            timeSeq.append(i / window_size)\n",
    "        timeSeq = np.array(timeSeq)\n",
    "        if np.ndim(Y) > 1:\n",
    "            Y = Y[:, 1]\n",
    "            Y = Y-500\n",
    "        Y = np.array(Y)\n",
    "        if method == \"zc\":\n",
    "            result = eye_movement_ZC(Y=Y, time=timeSeq)\n",
    "        elif method == \"max\":\n",
    "            result = eye_movement_max(Y=Y,time = timeSeq)\n",
    "        if result == None:\n",
    "            continue\n",
    "        ls_signals.append(result[\"signals\"])\n",
    "        ls_labels.append(result[\"labels\"])\n",
    "        ls_intervals.append(result[\"predictedInterval\"])\n",
    "    ls_labels = [item for sublist in ls_labels for item in sublist]\n",
    "    ls_signals = [item for sublist in ls_signals for item in sublist]\n",
    "    ls_intervals = [item for sublist in ls_intervals for item in sublist]\n",
    "    return {\n",
    "        \"ls_signals\":ls_signals,\n",
    "        \"ls_labels\":ls_labels,\n",
    "        \"ls_intervals\": ls_intervals\n",
    "    }\n",
    "\n",
    "# make a feature matrix for the following classifier\n",
    "def make_matrix(signals, labels):\n",
    "    mean_ls = []\n",
    "    sd_ls = []\n",
    "    zero_crossing = []\n",
    "    # entropy = []\n",
    "    # lumpiness = []\n",
    "    # flat_spots = []\n",
    "    for i in range(len(signals)):\n",
    "        mean = np.mean(signals[i])\n",
    "        zero_crossing.append(len(np.where(np.diff(np.sign(signals[i])))[0]))\n",
    "        sd = np.std(signals[i])\n",
    "        mean_ls.append(mean)\n",
    "        sd_ls.append(sd)\n",
    "        #entropy.append(ts_entropy(signals))\n",
    "        # lumpiness.append(ts_lumpiness(signals))\n",
    "        # flat_spots.append(ts_flat_spots(signals))\n",
    "    # , 'Signals': signals}\n",
    "    dependent_vars = pd.DataFrame({'Mean': mean_ls, 'SD': sd_ls})\n",
    "    feature_matrix = pd.concat([pd.Series(labels, name='Label'), dependent_vars], axis=1)\n",
    "    feature_matrix.to_csv(\"matrix.csv\", index=False)\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd83ce",
   "metadata": {},
   "source": [
    "## Get features using Tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237bf942",
   "metadata": {},
   "source": [
    "Get some useful features for each event using the Python package (Tsfresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ba5bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix_tsfresh(signals, labels):\n",
    "    df_list = []\n",
    "    for i, signal in enumerate(signals):\n",
    "        temp_df = pd.DataFrame({'id': i, 'time': np.arange(len(signal)), 'value': signal.astype(float)})\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    extracted_features = extract_features(df, column_id='id', column_sort='time',default_fc_parameters=MinimalFCParameters())\n",
    "    impute(extracted_features)\n",
    "    extracted_features['Label'] = labels\n",
    "    feature_matrix.to_csv(\"matrix.csv\", index=False)\n",
    "    return extracted_features\n",
    "\n",
    "#matrix = make_matrix_tsfresh(ls_signals,ls_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668c3b1",
   "metadata": {},
   "source": [
    "## Read matrix csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ad59065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_matrix_to_csv(feature_matrix, file_path):\n",
    "    feature_matrix.to_csv(file_path, index=False)\n",
    "\n",
    "def load_feature_matrix_from_csv(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6683395",
   "metadata": {},
   "source": [
    "# Different Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70aec0f",
   "metadata": {},
   "source": [
    "The next stage is to build up several classifiers using different algorithms. Now, Knn, Random Forest, Svm, Logistic Regression, Decision Tree and Gradient Boosting is included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a9860",
   "metadata": {},
   "source": [
    "## Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9982aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_knn(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(5)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, knn, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1a659",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "60b792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_rf(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, rf, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e3ada",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "269f85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_svm(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, svm, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b79ee",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd7dff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_dt(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, dt, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b34c71",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5a5393b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_lr(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    lr = LogisticRegression(random_state=42)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, lr, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93615f",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "68ec320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_gb(feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, cm, gb, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "11657752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(signals,classifier):\n",
    "    for i, signal in enumerate(signals):\n",
    "        temp_df = pd.DataFrame({'id': i, 'time': np.arange(len(signal)), 'value': signal.astype(float)})\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    extracted_features = extract_features(df, column_id='id', column_sort='time',default_fc_parameters=MinimalFCParameters())\n",
    "    impute(extracted_features)    \n",
    "    predicted_class = classifier.predict(extracted_features)\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f22df",
   "metadata": {},
   "source": [
    "## Write classifiers into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f0436db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classifier(classifier, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "def train_and_save_classifiers(classifiers, feature_matrix):\n",
    "    for name, classifier_func in classifiers.items():\n",
    "        accuracy, _, classifier, _, _ = classifier_func(feature_matrix)\n",
    "        save_classifier(classifier, f\"{name}_classifier.pkl\")\n",
    "        print(f\"{name} classifier trained and saved.\")\n",
    "\n",
    "def load_all_classifiers(filenames):\n",
    "    classifiers = {}\n",
    "    for filename in filenames:\n",
    "        classifier_name = filename[:-4].replace(\"_\", \" \")\n",
    "        classifiers[classifier_name] = load_classifier(filename)\n",
    "        print(f\"{classifier_name} classifier loaded.\")\n",
    "    return classifiers\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9ecd8",
   "metadata": {},
   "source": [
    "## Comparison of Different Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede2a70",
   "metadata": {},
   "source": [
    "## Simple Visualization of Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1872947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classifiers(classifiers, feature_matrix):\n",
    "    names, accuracies = [], []\n",
    "    for name, classifier_func in classifiers.items():\n",
    "        accuracy, _, _, _, _ = classifier_func(feature_matrix)\n",
    "        names.append(name)\n",
    "        accuracies.append(accuracy)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=names, y=accuracies)\n",
    "    plt.xlabel(\"Classifier\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Comparison of Classifier Accuracies\")\n",
    "    plt.show()\n",
    "\n",
    "classifiers = {\n",
    "    \"KNN\": classifier_knn,\n",
    "    \"Random Forest\": classifier_rf,\n",
    "    \"SVM\": classifier_svm,\n",
    "    \"Logistic Regression\": classifier_lr,\n",
    "    \"Decision Tree\": classifier_dt,\n",
    "    \"Gradient Boosting\": classifier_gb,\n",
    "}\n",
    "classifier_filenames = [\n",
    "    \"KNN_classifier.pkl\",\n",
    "    \"Random_Forest_classifier.pkl\",\n",
    "    \"SVM_classifier.pkl\",\n",
    "    \"Logistic_Regression_classifier.pkl\",\n",
    "    \"Decision_Tree_classifier.pkl\",\n",
    "    \"Gradient_Boosting_classifier.pkl\",\n",
    "]\n",
    "# compare_classifiers(classifiers, feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fa982",
   "metadata": {},
   "source": [
    "## Visualization of Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "831953c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(classifier, X_test, y_test):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plot_confusion_matrix(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7607e95",
   "metadata": {},
   "source": [
    "## Multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "707c7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_confusion_matrices(classifiers, feature_matrix):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    n_classifiers = len(classifiers)\n",
    "    nrows = 2\n",
    "    ncols = int(np.ceil(n_classifiers / nrows))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 5 * nrows))\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, (name, classifier_func) in zip(axes, classifiers.items()):\n",
    "        _, _, classifier, _, _ = classifier_func(feature_matrix)\n",
    "        plot_confusion_matrix(classifier, X_test, y_test, ax=ax, cmap=plt.cm.Blues)\n",
    "        ax.set_title(f\"{name} Confusion Matrix\")\n",
    "    \n",
    "    for i in range(n_classifiers, nrows * ncols):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ad31d",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "037e3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_classifiers_cross_validation(classifiers, feature_matrix, n_splits=5):\n",
    "    X = feature_matrix.drop('Label', axis=1)\n",
    "    y = feature_matrix['Label']\n",
    "    cv_scores_dict = {}\n",
    "    cv_times_dict = {}\n",
    "    \n",
    "    for name, classifier_func in classifiers.items():\n",
    "        accuracy, _, classifier, _, _ = classifier_func(feature_matrix)\n",
    "        cv_results = cross_validate(classifier, X, y, cv=n_splits, return_train_score=False)\n",
    "        cv_scores_dict[name] = cv_results['test_score']\n",
    "        cv_times_dict[name] = cv_results['score_time']\n",
    "        \n",
    "    cv_scores_df = pd.DataFrame(cv_scores_dict)\n",
    "    cv_times_df = pd.DataFrame(cv_times_dict)\n",
    "    \n",
    "    # Plot cross-validation scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=cv_scores_df)\n",
    "    plt.title(\"Classifier Cross-Validation Scores\")\n",
    "    plt.xlabel(\"Classifier\")\n",
    "    plt.ylabel(\"Cross-Validation Score\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot prediction times\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=cv_times_df)\n",
    "    plt.title(\"Classifier Prediction Times\")\n",
    "    plt.xlabel(\"Classifier\")\n",
    "    plt.ylabel(\"Prediction Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "    return cv_scores_dict, cv_times_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e42c4e",
   "metadata": {},
   "source": [
    "# Streaming Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "78f9245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_condition(knn_classifier, sample_rate, window_size=None, increment=None):\n",
    "    if window_size is None:\n",
    "        window_size = sample_rate\n",
    "    if increment is None:\n",
    "        increment = window_size // 3\n",
    "    input_buffer = []\n",
    "    lower_interval = 0\n",
    "    return_str = \"\"\n",
    "\n",
    "    while True:\n",
    "        new_data = read_data_from_spikerbox()\n",
    "        input_buffer.extend(new_data)\n",
    "        if len(input_buffer) >= lower_interval + window_size:\n",
    "            upper_interval = lower_interval + window_size\n",
    "            interval = np.array(input_buffer[lower_interval:upper_interval])\n",
    "            input_buffer = input_buffer[upper_interval:]\n",
    "            lower_interval = 0\n",
    "\n",
    "            mean = np.mean(interval)\n",
    "            sd = np.std(interval)\n",
    "            zero_crossing_point = len(np.where(np.diff(np.sign(interval)))[0])\n",
    "\n",
    "            if zero_crossing_point < 40:\n",
    "                temp_df = pd.DataFrame({'id': 0, 'time': np.arange(len(interval)), 'value': interval.astype(float)})\n",
    "                extracted_features = extract_features(temp_df, column_id='id', column_sort='time',\n",
    "                                                      default_fc_parameters=EfficientFCParameters())\n",
    "                impute(extracted_features)\n",
    "                features = extracted_features\n",
    "\n",
    "                predicted = knn_classifier.predict(features)\n",
    "                if predicted[0] == \"L\":\n",
    "                    goLeft()\n",
    "                elif predicted[0] == \"R\":\n",
    "                    goRight()\n",
    "                elif predicted[0] == \"B\":\n",
    "                    status = not status\n",
    "            else:\n",
    "                lower_interval = lower_interval + increment\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea5cbe",
   "metadata": {},
   "source": [
    "# Connect to Keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d8536a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goLeft():\n",
    "    pyautogui.press('left')\n",
    "\n",
    "def goRight():\n",
    "    pyautogui.press('right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20f1c7",
   "metadata": {},
   "source": [
    "# Visualization of Different Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f61bbcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/zoe_spiker/Length3/LLR_z.wav\n",
      "../datasets/zoe_spiker/Length3/RLL_z.wav\n",
      "../datasets/zoe_spiker/Length3/.DS_Store\n",
      "../datasets/zoe_spiker/Length3/RRR_z.wav\n",
      "../datasets/zoe_spiker/Length3/LRL_z.wav\n",
      "../datasets/zoe_spiker/Length3/RRL_z.wav\n",
      "../datasets/zoe_spiker/Length3/LRR_z.wav\n",
      "../datasets/zoe_spiker/Length3/RLR_z.wav\n",
      "../datasets/zoe_spiker/Length3/RRL_z2.wav\n",
      "../datasets/zoe_spiker/Length3/RRL_z3.wav\n",
      "../datasets/zoe_spiker/Length3/归档.zip\n",
      "../datasets/zoe_spiker/Length3/LLR_z3.wav\n",
      "../datasets/zoe_spiker/Length3/LLR_z2.wav\n",
      "../datasets/zoe_spiker/Length3/RLR_z2.wav\n",
      "../datasets/zoe_spiker/Length3/RLR_z3.wav\n",
      "../datasets/zoe_spiker/Length3/LRL_z3.wav\n",
      "../datasets/zoe_spiker/Length3/LRL_z2.wav\n",
      "../datasets/zoe_spiker/Length3/LLL_z3.wav\n",
      "../datasets/zoe_spiker/Length3/LLL_z2.wav\n",
      "../datasets/zoe_spiker/Length3/RRR_z2.wav\n",
      "../datasets/zoe_spiker/Length3/LRRz_2.wav\n",
      "../datasets/zoe_spiker/Length3/RRR_z3.wav\n",
      "../datasets/zoe_spiker/Length3/LRR_z3.wav\n",
      "../datasets/zoe_spiker/Length3/RLL_z2.wav\n",
      "../datasets/zoe_spiker/Length3/RLL_z3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|███████████████████| 20/20 [00:01<00:00, 10.44it/s]\n",
      "/Users/yche7179/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    value__sum_values  value__median  value__mean  value__length  \\\n",
      "0       -6.574972e+05     -50.868754   -57.924168        11351.0   \n",
      "1       -2.488980e+06    -306.868754  -257.898699         9651.0   \n",
      "2        6.044780e+05     -97.868754    60.745455         9951.0   \n",
      "3        1.476205e+06      95.000286    79.790567        18501.0   \n",
      "4       -1.473831e+06     -64.999714  -131.580287        11201.0   \n",
      "..                ...            ...          ...            ...   \n",
      "72      -1.483565e+06     -48.779874  -131.860756        11251.0   \n",
      "73      -1.195668e+06      17.220126   -91.265410        13101.0   \n",
      "74       2.528954e+06     293.730889   284.120171         8901.0   \n",
      "75      -1.880082e+06    -372.269111  -216.076490         8701.0   \n",
      "76      -2.047131e+06    -657.269111  -301.004444         6801.0   \n",
      "\n",
      "    value__standard_deviation  value__variance  value__root_mean_square  \\\n",
      "0                  776.634379     6.031610e+05               778.791479   \n",
      "1                  835.200502     6.975599e+05               874.111902   \n",
      "2                 1260.831078     1.589695e+06              1262.293554   \n",
      "3                 1193.025489     1.423310e+06              1195.690743   \n",
      "4                  858.980940     7.378483e+05               869.000361   \n",
      "..                        ...              ...                      ...   \n",
      "72                 657.057078     4.317240e+05               670.157640   \n",
      "73                 749.732677     5.620991e+05               755.267146   \n",
      "74                1678.920930     2.818775e+06              1702.791755   \n",
      "75                 897.727786     8.059152e+05               923.365706   \n",
      "76                1148.642071     1.319379e+06              1187.426749   \n",
      "\n",
      "    value__maximum  value__absolute_maximum  value__minimum Label  \n",
      "0      1363.131246              2452.868754    -2452.868754     L  \n",
      "1      1286.131246              2209.868754    -2209.868754     L  \n",
      "2      3241.131246              3241.131246    -1989.868754     R  \n",
      "3      3724.000286              3724.000286    -2915.999714     R  \n",
      "4      1279.000286              2539.999714    -2539.999714     L  \n",
      "..             ...                      ...             ...   ...  \n",
      "72      907.220126              1671.779874    -1671.779874     L  \n",
      "73     1220.220126              2547.779874    -2547.779874     L  \n",
      "74     4243.730889              4243.730889    -1975.269111     R  \n",
      "75     1501.730889              1984.269111    -1984.269111     L  \n",
      "76     1368.730889              2772.269111    -2772.269111     L  \n",
      "\n",
      "[77 rows x 11 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=5.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [144]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m matrix \u001b[38;5;241m=\u001b[39m make_matrix_tsfresh(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mls_signals\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mls_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(matrix)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_and_save_classifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36mtrain_and_save_classifiers\u001b[0;34m(classifiers, feature_matrix)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_save_classifiers\u001b[39m(classifiers, feature_matrix):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, classifier_func \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 6\u001b[0m         accuracy, _, classifier, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         save_classifier(classifier, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_classifier.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classifier trained and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [130]\u001b[0m, in \u001b[0;36mclassifier_knn\u001b[0;34m(feature_matrix)\u001b[0m\n\u001b[1;32m      6\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> 8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m     10\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:214\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m        Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    216\u001b[0m     _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:717\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    715\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     query_is_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:761\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_2d:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;66;03m# If input is scalar raise error\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    762\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got scalar array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    766\u001b[0m         )\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=5.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load the classifier\n",
    "    path = \"../datasets/zoe_spiker/Length3\"\n",
    "    file_ls = []\n",
    "    wave_file_ls = os.listdir(path)\n",
    "    for i in range(len(wave_file_ls)):\n",
    "        file_path = path + \"/\" + wave_file_ls[i]\n",
    "        file_ls.append(file_path)\n",
    "    results = record_all_training(files = file_ls,method=\"zc\")\n",
    "\n",
    "    if results != None:\n",
    "        matrix = make_matrix_tsfresh(results[\"ls_signals\"], results[\"ls_labels\"])\n",
    "        print(matrix)\n",
    "        train_and_save_classifiers(classifiers, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49579f",
   "metadata": {},
   "source": [
    "## Load classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = results[\"ls_signals\"]\n",
    "classifiers = load_all_classifiers(classifier_filenames)\n",
    "rf = classifiers['Random Forest classifier']\n",
    "# rf.predict(signals)\n",
    "predict(signals,rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(signals,classifier):\n",
    "    for i, signal in enumerate(signals):\n",
    "        df_list = []\n",
    "        temp_df = pd.DataFrame({'id': i, 'time': np.arange(len(signal)), 'value': signal.astype(float)})\n",
    "        df_list.append(temp_df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    extracted_features = extract_features(df, column_id='id', column_sort='time',default_fc_parameters=EfficientFCParameters())\n",
    "    impute(extracted_features)\n",
    "    predicted_class = classifier.predict(extracted_features)\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac16bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_classifiers(classifiers, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, cm, svm_classifier, X_test, y_test = classifier_svm(matrix)\n",
    "visualize_confusion_matrix(svm_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40135e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_confusion_matrices(classifiers, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_dict = all_classifiers_cross_validation(classifiers, matrix, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a57f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores, cv_times = all_classifiers_cross_validation(classifiers, matrix, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_csv(\"test_feature.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
